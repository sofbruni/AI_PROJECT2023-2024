{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import numpy and matplot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ybz2FvWDipey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans"
      ],
      "metadata": {
        "id": "iwe2euiYjZh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and preprocessing"
      ],
      "metadata": {
        "id": "gNdTB-_mofED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "n_samples = 500\n",
        "\n",
        "blobs_coord, blob_classes = make_blobs(n_samples = n_samples, random_state = 2112)\n",
        "\n",
        "X = blobs_coord.copy()\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_scaled[:,0], X_scaled[:,1])\n",
        "ax.set_title(\"Scaled coordinate from make_blob dataset\")"
      ],
      "metadata": {
        "id": "0tkrsV28jY5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting"
      ],
      "metadata": {
        "id": "qOtOYZIHohyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 3\n",
        "clustering_method = KMeans(n_clusters = n_clusters, init = 'random', n_init = 'auto') # init = {'k-means++', 'random'}\n",
        "clustering_method.fit(X)\n",
        "predicted_classes = clustering_method.labels_\n",
        "unique_labels = np.unique(predicted_classes)\n",
        "fig, ax = plt.subplots()\n",
        "for cls in unique_labels:\n",
        "  class_indices = predicted_classes == cls\n",
        "  ax.scatter(X_scaled[class_indices, 0], X_scaled[class_indices, 1])"
      ],
      "metadata": {
        "id": "JpRyKNh7nV7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: inertia\n",
        "Inertia is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster.\n",
        "A good model is one with low inertia AND a low number of clusters (K).\n",
        "Inertia is measured by:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "clustering_method.inertia_\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vZ3JC58jojcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters_vector = [i for i in range(1,10)]\n",
        "inertia_vector = []\n",
        "\n",
        "for n_clusters in n_clusters_vector:\n",
        "  clustering_method = KMeans(n_clusters = n_clusters, init = 'random', n_init = 'auto') # init = {'k-means++', 'random'}\n",
        "  clustering_method.fit(X)\n",
        "  inertia_vector.append(clustering_method.inertia_)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(n_clusters_vector, inertia_vector)\n",
        "ax.scatter(n_clusters_vector, inertia_vector)\n",
        "ax.set_title('Inertia of model vs. different n_clusters')\n",
        "ax.set_xlabel('n_clusters')\n",
        "ax.set_ylabel('Inertia')\n",
        "ax.grid()\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "kn6p_vfyoGCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From n_clusters = 3 on the inertia change its slope -> n_clusters = 3 is the best value."
      ],
      "metadata": {
        "id": "JUiZxgqTp7bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: Mean Silhouette Coefficient\n",
        "\n",
        "$s = \\frac{b - a}{max(b - a)}$\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "*   $a$ is the average distance between one data point and all other points in the same cluster\n",
        "\n",
        "*   $b$ is the  average distance between one data point and all other points in the next nearest cluster\n"
      ],
      "metadata": {
        "id": "7zGHnsjNwmBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(silhouette_score(X_scaled, predicted_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvRw-sd_w6Dd",
        "outputId": "17d76590-436f-4cb3-b7d4-3bb7e58c416e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8405614394495962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical clustering"
      ],
      "metadata": {
        "id": "matvHwnpqHsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and preprocessing"
      ],
      "metadata": {
        "id": "cbUbHzvJqMjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_blobs\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "n_samples = 500\n",
        "\n",
        "blobs_coord, blob_classes = make_blobs(n_samples = n_samples, random_state = 2112)\n",
        "\n",
        "X = blobs_coord.copy()\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_scaled[:,0], X_scaled[:,1])\n",
        "ax.set_title(\"Scaled coordinate from make_blob dataset\")"
      ],
      "metadata": {
        "id": "PfjPZai4pciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting"
      ],
      "metadata": {
        "id": "vl9pftMQqyK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linkage functions\n",
        "<img src='https://i0.wp.com/dataaspirant.com/wp-content/uploads/2020/12/15-Hierarchical-Clustering-Linkages.png?resize=609%2C659&ssl=1'>"
      ],
      "metadata": {
        "id": "j6WdUZ2SrwbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendrogram to find the optimal number of clusters\n",
        "fig, ax = plt.subplots()\n",
        "linkage_method = sch.linkage(X_scaled, method = 'ward', metric = 'euclidean') # method = {'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward'}\n",
        "dendrogram = sch.dendrogram(linkage_method, ax = ax)\n",
        "ax.set_title('Dendrogram')\n",
        "ax.set_xlabel('Point index in the make_blob dataset')\n",
        "ax.set_ylabel('Euclidean distances')\n",
        "ax.set_xticks([])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KQlwvejcqzDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hierarchy_model = AgglomerativeClustering(n_clusters = 3, metric = 'euclidean', linkage = 'ward')\n",
        "y_predicted = hierarchy_model.fit_predict(X)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "unique_labels = np.unique(y_predicted)\n",
        "\n",
        "# Visualising the clusters\n",
        "for cls in unique_labels:\n",
        "  ax.scatter(X_scaled[y_predicted == cls, 0], X_scaled[y_predicted == cls, 1], label = f'Cluster {cls}')\n",
        "\n",
        "ax.set_title('Clusters make_blobs')\n",
        "ax.set_xlabel(r'$X_0$')\n",
        "ax.set_ylabel(r'$X_1$')\n",
        "ax.legend()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-LGmQe1Hte0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DBSCAN\n",
        "\n",
        "<img src='https://www.researchgate.net/publication/342082665/figure/fig2/AS:903773622898690@1592487831444/The-DBSCAN-algorithm-and-two-generated-clusters-There-are-three-types-of-points-as.png'>"
      ],
      "metadata": {
        "id": "enCbQAbTuhn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and preprocessing\n",
        "\n",
        "make moons!"
      ],
      "metadata": {
        "id": "Rx4hAoz6uyOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "n_samples = 500\n",
        "\n",
        "blobs_coord, blob_classes = make_moons(n_samples=n_samples, noise=0.05, random_state = 5150)\n",
        "\n",
        "X = blobs_coord.copy()\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_scaled[:,0], X_scaled[:,1])\n",
        "ax.set_title(\"Scaled coordinate from make_blob dataset\")"
      ],
      "metadata": {
        "id": "b5k3-AMouzUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "clustering_method = DBSCAN(eps = 0.15, min_samples = 5) #eps = 0.5, 0.15\n",
        "clustering_method.fit(X_scaled)\n",
        "predicted_classes = clustering_method.labels_\n",
        "unique_labels = np.unique(predicted_classes)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for cls in unique_labels:\n",
        "  class_indices = predicted_classes == cls\n",
        "  ax.scatter(X_scaled[class_indices, 0], X_scaled[class_indices, 1])\n",
        "\n"
      ],
      "metadata": {
        "id": "mFAlnA_ju3TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method may be a powerful one if your goal is to deal with outliners and non-linearity."
      ],
      "metadata": {
        "id": "KG0PgXePzWSO"
      }
    }
  ]
}